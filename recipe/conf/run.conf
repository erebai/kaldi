# begin configuration section
data_preparation=false
preparation_stage=7
training_decoding=true
stage_tr=7
one_stage=true
#==================< DATA PREPARATION >================
adapt=false # Set this to true if you want to make the data as the vocabulary file,
	    # example: dès que (original text) => dès_que (vocabulary word)
liaison=true # Set this to true if you want to makes lexicon while taking into account liaison for French language
sample_rate=16000
#set the path to the training, development, and evaluation folders
  data_train="speech/test" # directory which contains the training dataset
		     # To use multiple data source for training, use space " " as a delimiter between each dataset
  data_dev=""
  data_test="speech/test"
  tgt_dir=data_dir # The folder in which the generated files will be saved
#set dictionnary path
sequiture_model=conf/model-2
lexicon=lexicon
#set the language model parameters
lms_function=( KALDI )
lms_order=( 3 )
lms=( KALDI )
lms_lambda=( 1 )
train_text=data/local/dict/corpus # LM Training file. It's the training transcription by default
perplexity_file=perplexity.txt

#NOTE: if you would like to use a pre-compiled Language Model, just pute the file in the lm directory: data/local/lm/


#set the kaldi language directories parameters
sil_prob=0.3 # silence probability used while creating l.fst (transition probability from silence state to the loop state)

#set feature type: mfcc, plp, or fbank
feat_type=mfcc
feat_nj=10
#==================< DATA PREPARATION END >================

#==================< TRAINING AND EVALUATION >================
#Global parameters
exp_dir=exp
train_nj=4
decode_nj=5
data_decode="dev test"
decode_lms=( IRSTLM SRILM )
decode_conf=conf/decode.config
decode_dnn_conf=conf/decode_dnn.config
save_results_file=RESULTS.txt
context_opts="--context-width=3 --central-position=1" # triphone context, e.g. "--context-width 5 --central-position 2" for quinphone.
sub_train_data=200
#set mono parameters
sub_data=100
#set Tri1 parameters
numLeavesTri1=2500
numGaussTri1=15000
#set LDA-MLLR parameters
numLeavesMLLT=2500
numGaussMLLT=15000
sliceTri2="--left-context=3 --right-context=3"
#set SAT-fmllr parameters
numLeavesSAT=2500
numGaussSAT=15000
#set SGMM2 parameters
numGaussUBM=400
numLeavesSGMM=7000
numGaussSGMM=9000
#set DNN parameters
DNN_technique="dbn" #Define the neural net technique: dnn|dbn|autoencoder
# get more information about how to set DNN nnet2 parameters: http://kaldi-asr.org/doc/dnn2.html

############### dnn configuration ################
use_gpu=true
train_stage=-100

if $use_gpu; then
  num_threads=1
  parallel_opts="--gpu 1"
  minibatch_size=512
else
  # with just 4 jobs this might be a little slow.
  num_threads=16
  parallel_opts="--num-threads $num_threads" 
  minibatch_size=128
fi

samples_per_iter=400000
initial_learning_rate=0.01
final_learning_rate=0.001
mix_up=8000

hidden_function=Pnorm # The implemented functions are: Pnorm, Tanh, Sigmoid, RectifiedLinear 
num_hidden_layers=5
hidden_layer_dim=128 #used with Tanh, Sigmoid, RectifiedLinear activation function
pnorm_input_dim=2000 #used with pnorm function
pnorm_output_dim=400 #used with pnorm function

############### dbn configuration ################
data_fmllr=data-fmllr
depth=7
learn_rate=0.008
stage_dbn=0
#==================< TRAINING AND EVALUATION END >================

# end configuration section
